{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/Qse96iZdyj9FL9plJe1O",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Niroth36/Convolutional_Neural_Networks_CRCslides_dataset/blob/main/CRC_slides_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing the libraries we need"
      ],
      "metadata": {
        "id": "U_0mJJyFcZw1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import seaborn as sns"
      ],
      "metadata": {
        "id": "Cu93xjnzcZEl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mounting drive to use CRC_slides dataset"
      ],
      "metadata": {
        "id": "rvZsMaWzdWnT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JSP3jgg0Y8r2",
        "outputId": "d6d927c1-f3e3-4d97-b6b9-6c568e22d83f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "cp: cannot stat '/content/drive/MyDrive/datasets/CRC_slides.tar.gz': No such file or directory\n",
            "tar (child): CRC_slides.tar.gz: Cannot open: No such file or directory\n",
            "tar (child): Error is not recoverable: exiting now\n",
            "tar: Child returned status 2\n",
            "tar: Error is not recoverable: exiting now\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!cp '/content/drive/MyDrive/datasets/CRC_slides.tar.gz' .\n",
        "!tar -xvzf 'CRC_slides.tar.gz'\n",
        "data_dir = '/content/CRC_slides'"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "def load_dataset(data_dir, train_pct=0.6, val_pct=0.2, test_pct=0.2, batch_size=64, img_size=(224, 224)):\n",
        "\n",
        "    # Create a list of all the image file paths in the data directory\n",
        "    image_paths = tf.data.Dataset.list_files(data_dir + '/*/*')\n",
        "\n",
        "    # Get the class names from the directory structure\n",
        "    classes = list(map(lambda x: x.split(\"/\")[-2],image_paths.as_numpy_iterator()))\n",
        "\n",
        "    # Split the data into train, validation, and test sets\n",
        "    train_data,test_data,train_label,test_label = train_test_split(image_paths, classes, train_size=train_pct,test_size=test_pct)\n",
        "    val_data,test_data,val_label,test_label = train_test_split(test_data, test_label, train_size=val_pct/(val_pct+test_pct),test_size=test_pct/(val_pct+test_pct))\n",
        "\n",
        "    # Convert the data to a tf.data.Dataset\n",
        "    train_ds = tf.data.Dataset.from_tensor_slices((train_data, train_label))\n",
        "    val_ds = tf.data.Dataset.from_tensor_slices((val_data, val_label))\n",
        "    test_ds = tf.data.Dataset.from_tensor_slices((test_data, test_label))\n",
        "    devel_ds = tf.data.Dataset.from_tensor_slices((val_data, val_label))\n",
        "\n",
        "    # Apply image preprocessing and resizing\n",
        "    train_ds = train_ds.map(lambda x, y: (tf.image.resize(tf.image.decode_jpeg(tf.io.read_file(x)), img_size), y))\n",
        "    val_ds = val_ds.map(lambda x, y: (tf.image.resize(tf.image.decode_jpeg(tf.io.read_file(x)), img_size), y))\n",
        "    test_ds = test_ds.map(lambda x, y: (tf.image.resize(tf.image.decode_jpeg(tf.io.read_file(x)), img_size), y))\n",
        "    devel_ds = devel_ds.map(lambda x, y: (tf.image.resize(tf.image.decode_jpeg(tf.io.read_file(x)), img_size), y))\n",
        "\n",
        "    # Apply data augmentation\n",
        "    train_ds = train_ds.map(lambda x, y: (tf.image.random_flip_left_right(x), y))\n",
        "    train_ds = train_ds.map(lambda x, y: (tf.image.random_brightness(x, max_delta=0.5), y))\n",
        "    train_ds = train_ds.map(lambda x, y: (tf.image.random_contrast(x, lower=0.5, upper=1.5), y))\n",
        "\n",
        "    # Normalize images\n",
        "    train_ds = train_ds.map(lambda x, y: (tf.image.per_image_standardization(x), y))\n",
        "    val_ds = val_ds.map(lambda x, y: (tf.image.per_image_standardization(x), y))\n",
        "    test_ds = test_ds.map(lambda x, y: (tf.image.per_image_standardization(x), y))\n",
        "    devel_ds = devel_ds.map(lambda x, y: (tf.image.per_image_standardization(x), y))\n",
        "\n",
        "    # create a mapping from class names to integers\n",
        "    unique_labels = list(set(classes))\n",
        "    label_to_index = {label: index for index, label in enumerate(unique_labels)}\n",
        "\n",
        "    # Map labels to integers\n",
        "    train_ds = train_ds.map(lambda x, y: (x, label_to_index[y]))\n",
        "    val_ds = val_ds.map(lambda x, y: (x, label_to_index[y]))\n",
        "    test_ds = test_ds.map(lambda x, y: (x, label_to_index[y]))\n",
        "    devel_ds = devel_ds.map(lambda x, y: (x, label_to_index[y]))\n",
        "\n",
        "    # Shuffle and batch the data\n",
        "    train_ds = train_ds.shuffle(buffer_size=1000).batch(batch_size)\n",
        "    val_ds = val_ds.batch(batch_size)\n",
        "    test_ds = test_ds.batch(batch_size)\n",
        "    devel_ds = devel_ds.batch(batch_size)\n",
        "    \n",
        "    # Return the datasets\n",
        "    return devel_ds, train_ds, val_ds, test_ds, unique_labels\n"
      ],
      "metadata": {
        "id": "-4i4BhQAdsbY"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "devel_ds, train_ds, val_ds, test_ds, classes = load_dataset(data_dir, train_pct=0.6, val_pct=0.2, test_pct=0.2, batch_size=64, img_size=(224, 224))"
      ],
      "metadata": {
        "id": "v_7kooO0gh1T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "y = np.concatenate([y for x, y in devel_ds])\n",
        "plt.hist(y, list(range(len(classes) + 1)))\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "y10KalTneDmJ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}